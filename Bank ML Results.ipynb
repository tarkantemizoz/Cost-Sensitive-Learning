{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bayes_opt.bayes_logistic import bayes_logistic\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from bayes_opt.bayes_tree import bayes_tree\n",
    "from bayes_opt.bayes_xgboost import bayes_xgboost\n",
    "from bayes_opt.utils import test_learning\n",
    "\n",
    "\n",
    "train_sets = pickle.load(open(\"datasets/train_sets_bank.dat\",\"rb\"))\n",
    "test_sets = pickle.load(open(\"datasets/test_sets_bank.dat\",\"rb\"))\n",
    "val_sets = [pickle.load(open(\"datasets/val_sets_bank.dat\",\"rb\")), pickle.load(open(\"datasets/val_test_sets_bank.dat\",\"rb\"))]\n",
    "features = pickle.load(open(\"datasets/bank_features\",\"rb\"))\n",
    "returns = pickle.load(open(\"datasets/bank_returns\",\"rb\"))\n",
    "outcomes = pickle.load(open(\"datasets/bank_outcomes\",\"rb\"))\n",
    "rmax = np.zeros(len(returns))\n",
    "for i in range(len(returns)):\n",
    "    rmax[i] = returns[i,np.argmax(returns[i])]\n",
    "    \n",
    "replication = 5\n",
    "n_splits = 10\n",
    "n_steps_xgb = 20\n",
    "n_trials = 50\n",
    "results = []\n",
    "\n",
    "for rep in range(replication):\n",
    "\n",
    "    bank_ml_results = np.zeros((n_splits, 10))\n",
    "\n",
    "    for i in range(n_splits):\n",
    "\n",
    "        config = {\n",
    "            \"cval\": True,\n",
    "            \"rep\" : rep,\n",
    "            \"set\" : i,\n",
    "            \"n_trials\" : n_trials,\n",
    "            \"val_sets\": val_sets,\n",
    "            \"n_steps\": n_steps_xgb\n",
    "            }    \n",
    "            \n",
    "        scaler = MinMaxScaler()\n",
    "        x_train_scaled = scaler.fit_transform(features[train_sets[rep][i]])\n",
    "        x_test_scaled = scaler.transform(features[test_sets[rep][i]])            \n",
    "            \n",
    "        bayes = bayes_logistic(config, x_train_scaled, outcomes[train_sets[rep][i]], returns[train_sets[rep][i]])\n",
    "        best_params_logistic = bayes.bayes()\n",
    "        c, penalty = best_params_logistic.get(\"c\", \"\"), best_params_logistic.get(\"penalty\", \"\")\n",
    "        clf = (LogisticRegression(C = c, penalty=penalty, solver='saga') if penalty != \"elasticnet\"\n",
    "               else LogisticRegression(C = c, penalty=penalty, solver='saga', l1_ratio = best_params_logistic.get(\"l1\", \"\")))          \n",
    "        clf_fit = clf.fit(x_train_scaled, outcomes[train_sets[rep][i]])\n",
    "        probs = clf_fit.predict_proba(x_test_scaled)\n",
    "        test_return, test_outcome = test_learning(probs, returns[test_sets[rep][i]])\n",
    "        bank_ml_results[i, 0] = test_return / sum(rmax[test_sets[rep][i]])\n",
    "        bank_ml_results[i, 3] = accuracy_score(outcomes[test_sets[rep][i]], test_outcome)\n",
    "        bank_ml_results[i, 4] = f1_score(outcomes[test_sets[rep][i]], test_outcome)\n",
    "            \n",
    "        bayes = bayes_tree(config, features[train_sets[rep][i]], outcomes[train_sets[rep][i]], returns[train_sets[rep][i]])\n",
    "        best_params_tree = bayes.bayes()\n",
    "        depth, min_samples, cp = best_params_tree.get(\"max_depth\", \"\"), best_params_tree.get(\"min_samples_leaf\", \"\"), best_params_tree.get(\"ccp_alpha\", \"\")\n",
    "        dt = DecisionTreeClassifier(min_samples_leaf = min_samples, ccp_alpha = cp, max_depth = depth).fit(features[train_sets[rep][i]], outcomes[train_sets[rep][i]])  \n",
    "        probs = dt.predict_proba(features[test_sets[rep][i]])\n",
    "        test_return, test_outcome = test_learning(probs, returns[test_sets[rep][i]])\n",
    "        bank_ml_results[i, 1] = test_return / sum(rmax[test_sets[rep][i]])\n",
    "        bank_ml_results[i, 5] = accuracy_score(outcomes[test_sets[rep][i]], test_outcome)\n",
    "        bank_ml_results[i, 6] = f1_score(outcomes[test_sets[rep][i]], test_outcome)      \n",
    "            \n",
    "        bayes = bayes_xgboost(config, features[train_sets[rep][i]], outcomes[train_sets[rep][i]], returns[train_sets[rep][i]])\n",
    "        best_params_xgb = bayes.bayes()            \n",
    "        param = {'eta' : best_params_xgb.get(\"eta\", \"\"), \n",
    "                 'max_depth' : best_params_xgb.get(\"max_depth\", \"\"),\n",
    "                 'min_child_weight' : best_params_xgb.get(\"min_child_weight\", \"\"),\n",
    "                 'gamma' : best_params_xgb.get(\"gamma\", \"\"),\n",
    "                 'colsample_bytree' : best_params_xgb.get(\"colsample_bytree\", \"\"),\n",
    "                 'objective': 'multi:softprob',\n",
    "                 'num_class': 2 }                 \n",
    "        model = xgb.train(param, xgb.DMatrix(features[train_sets[rep][i]], label=outcomes[train_sets[rep][i]]), config[\"n_steps\"])                      \n",
    "        probs = model.predict(xgb.DMatrix(features[test_sets[rep][i]], label=outcomes[test_sets[rep][i]]))\n",
    "        test_return, test_outcome = test_learning(probs, returns[test_sets[rep][i]])\n",
    "        bank_ml_results[i, 2] = test_return / sum(rmax[test_sets[rep][i]])\n",
    "        bank_ml_results[i, 7] = accuracy_score(outcomes[test_sets[rep][i]], test_outcome)\n",
    "        bank_ml_results[i, 8] = f1_score(outcomes[test_sets[rep][i]], test_outcome)   \n",
    "    \n",
    "    results.append(bank_ml_results)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import optuna\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from Models.linearnet import LinearNet\n",
    "from Models.opt_torch import Optimization\n",
    "from bayes_opt.utils import give_set, give_score, test_learning\n",
    "\n",
    "class cslr_bayes: \n",
    "\n",
    "    def __init__(self, data, returns, config):\n",
    "        \n",
    "        self.device = config.get(\"device\", \"cpu\")\n",
    "        self.cval = config.get(\"cval\", False)\n",
    "        self.n_val_splits = config.get(\"n_val_splits\", 10)    \n",
    "        self.rep = config.get(\"rep\", 0) \n",
    "        self.set_num = config.get(\"set\", 0) \n",
    "        self.n_trials = config.get(\"n_trials\", 2)\n",
    "        self.val_sets = config.get(\"val_sets\", None)\n",
    "        self.batchnorm = config.get(\"batchnorm\", False)\n",
    "        self.n_epochs = config.get(\"n_epochs\", 2000)\n",
    "        self.n_steps = config.get(\"n_steps\", 1000)        \n",
    "        self.batches = config.get(\"batches\", None)\n",
    "        self.batch_size = config.get(\"batch_size\", 1000)\n",
    "        self.max_batch = config[\"max_batch\"]\n",
    "        self.layers = config.get(\"layers\", None)        \n",
    "        self.numlayer = config.get(\"dnn_layers\", 1)    \n",
    "        self.hidden_size = config.get(\"hidden_size\", [])\n",
    "        self.lr = config.get(\"lr\", None)   \n",
    "        self.lr_rate = config.get(\"lr_rate\", 0.001)    \n",
    "        self.data = data\n",
    "        self.returns = returns\n",
    "    \n",
    "    def train_bayes_linear(self, trial):                 \n",
    "                  \n",
    "        space = {'lr_rate' : (trial.suggest_uniform('lr_rate', 0.00005, 0.01) if self.lr is not None else self.lr_rate)\n",
    "                'batch_size': (trial.suggest_int('batch_size', self.batches[0], self.batches[1]) if self.batches is not None\n",
    "                               else self.batch_size),\n",
    "                 'numlayer': (trial.suggest_int('layers', self.layers[0], self.layers[1]) if self.layers is not None\n",
    "                              else self.numlayer)\n",
    "                }\n",
    "              \n",
    "        config_nn = {\n",
    "            \"n_inputs\" : self.data.shape[1],\n",
    "            \"dnn_layers\" : space['numlayer'],\n",
    "            \"hidden_size\" : self.hidden_size,\n",
    "            \"n_outputs\" : self.returns.shape[1],\n",
    "            \"batchnorm\" : self.batchnorm,\n",
    "            \"n_epochs\": self.n_epochs,\n",
    "            \"n_steps\": self.n_steps\n",
    "        }\n",
    "        \n",
    "        if self.cval == False:\n",
    "              \n",
    "            model = LinearNet(config_nn).to(self.device)\n",
    "            config_nn[\"batch_size\"] = (2**space['batch_size'] if space['batch_size']<self.max_batch else len(self.data))                                                \n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=space['lr_rate'])\n",
    "            optimization = Optimization(model, optimizer, config_nn)\n",
    "            optimization.train(self.data, self.returns)\n",
    "            _, _, test_probs = optimization.evaluate(self.data, self.returns)\n",
    "            test_return = test_learning(test_probs, returns)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            test_return = np.zeros(self.n_val_splits)\n",
    "            \n",
    "            if self.val_sets is None:\n",
    "                skf = KFold(self.n_val_splits, shuffle=True)\n",
    "                for train_index, test_index in skf.split(self.data):\n",
    "                    x_train, x_test = self.data[train_index], self.data[test_index]\n",
    "                    r_train, r_test = self.returns[train_index], self.returns[test_index]\n",
    "                    model = LinearNet(config_nn).to(self.device)\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=space['lr_rate'])\n",
    "                    config_nn[\"batch_size\"] = (2**space['batch_size'] if space['batch_size']<self.max_batch else len(x_train))                                    \n",
    "                    optimization = Optimization(model, optimizer, config_nn)\n",
    "                    optimization.train(x_train, r_train, x_test, r_test)\n",
    "                    _, _, test_probs = optimization.evaluate(x_test, r_test)\n",
    "                    test_return[i],_ = test_learning(test_probs, r_test)            \n",
    "            \n",
    "            else:\n",
    "                for i in range(self.n_val_splits):\n",
    "                    train_index, test_index = give_set(self.rep, self.set_num, i, self.val_sets)\n",
    "                    x_train, x_test = self.data[train_index], self.data[test_index]\n",
    "                    r_train, r_test = self.returns[train_index], self.returns[test_index]\n",
    "\n",
    "                    model = LinearNet(config_nn).to(self.device)\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=space['lr_rate'])\n",
    "                    config_nn[\"batch_size\"] = (2**space['batch_size'] if space['batch_size']<self.max_batch else len(x_train))                \n",
    "                    optimization = Optimization(model, optimizer, config_nn)\n",
    "                    optimization.train(x_train, r_train, x_test, r_test)\n",
    "                    _, _, test_probs = optimization.evaluate(x_test, r_test)\n",
    "                    test_return[i],_ = test_learning(test_probs, r_test)\n",
    "        \n",
    "        return test_return.mean()\n",
    "\n",
    "    def bayes(self):\n",
    "\n",
    "        sampler = optuna.samplers.TPESampler()    \n",
    "        study = optuna.create_study(sampler=sampler, direction='maximize')\n",
    "        study.optimize(func=self.train_bayes_linear, n_trials=self.n_trials)\n",
    "    \n",
    "        return study.best_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from bayes_opt.bayes_linearnet import cslr_bayes\n",
    "from Models.linearnet import LinearNet\n",
    "from Models.opt_torch import Optimization\n",
    "from Models.gurobi_opt import Optimization_MIP\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from gurobipy import *\n",
    "\n",
    "\n",
    "class cslr: \n",
    "\n",
    "    def __init__(self, config, x_train, r_train, x_test, r_test, x_val=None, r_val=None):\n",
    "        self.device = config.get(\"device\", \"cpu\")\n",
    "        self.n_epochs = config.get(\"n_epochs\", 1000)\n",
    "        self.n_steps = config.get(\"n_steps\", \"500\")    \n",
    "        self.lr_rate = config.get(\"lr_rate\", 5e-3)   \n",
    "        self.hidden_size = config.get(\"hidden_size\", 0)\n",
    "        self.batch_size = config.get(\"batch_size\", 1000)\n",
    "        self.num_layer = config.get(\"numlayer\", 1)\n",
    "        self.batchnorm = config.get(\"batchnorm\", False)\n",
    "        self.scaler = config.get(\"scaler\", None)\n",
    "        self.batches_bayes = config.get(\"batches\", None)\n",
    "        self.lr_bayes = config.get(\"lr\", None)\n",
    "        self.layers_bayes = config.get(\"lr\", None)\n",
    "        self.bayes = config.get(\"bayes\", False)\n",
    "        self.rep = config.get(\"rep\", None)\n",
    "        self.set = config.get(\"set\", None)\n",
    "        self.cval = config.get(\"cval\", None)\n",
    "        self.n_val_splits = config.get(\"n_val_splits\", 10)\n",
    "        self.val_sets = config.get(\"val_sets\", None)\n",
    "        self.n_trials = config.get(\"n_trials\", None)   \n",
    "        self.time_limit = config.get(\"time_limit\", 100.0)\n",
    "        self.give_initial = config.get(\"initial\", False)        \n",
    "        self.beta_initial = None\n",
    "        self.x_train = x_train\n",
    "        self.x_test = x_test\n",
    "        self.r_train = r_train\n",
    "        self.r_test = r_test\n",
    "        self.x_val = x_val\n",
    "        self.r_val = r_val\n",
    "        self.max_batch = config.get(\"max_batch\", len(self.x_train))\n",
    "        self.num_class = r_train.shape[1]\n",
    "        self.num_features = x_train.shape[1]\n",
    "        self.model = None\n",
    "        \n",
    "    def opt_initial(self, beta_opt):\n",
    "        \n",
    "        scores = np.zeros((len(self.x_train), self.num_class))\n",
    "        scores_diff = np.zeros((len(self.x_train), ((self.num_class * (self.num_class - 1)) // 2)))\n",
    "        for i in range(len(self.x_train)):\n",
    "            for k in range(self.num_class):\n",
    "                scores[i,k] = sum(self.x_train[i,j] * beta_opt[k,j].item() for j in range(self.num_features))\n",
    "        diff_ind = 0\n",
    "        for k in range(self.num_class-1):\n",
    "            for t in range(self.num_class-(k+1)):\n",
    "                scores_diff[:,diff_ind] = np.subtract(scores[:,k], scores[:,(k+t+1)])\n",
    "                diff_ind += 1\n",
    "        is_deviate = []\n",
    "        for i in range(len(scores_diff)):\n",
    "            if (abs(scores_diff[i,:]) < 0.01).any() or (scores[i,:] > 100).any():\n",
    "                is_deviate.append(i)\n",
    "        return  np.delete(self.x_train, is_deviate, 0), np.delete(self.r_train, is_deviate, 0)     \n",
    "    \n",
    "    def gradient(self):\n",
    "        \n",
    "        if self.scaler is not None:\n",
    "            scaler = self.scaler\n",
    "            self.x_train = scaler.fit_transform(self.x_train)\n",
    "            self.x_test = scaler.transform(self.x_test)\n",
    "            if self.x_val is not None:\n",
    "                self.x_val = scaler.transform(self.x_val)\n",
    "        x_train_nn = torch.Tensor(self.x_train).to(self.device)   \n",
    "        x_test_nn = torch.Tensor(self.x_test).to(self.device)\n",
    "        r_train_nn = torch.Tensor(self.r_train).to(self.device)\n",
    "        r_test_nn = torch.Tensor(self.r_test).to(self.device)\n",
    "        config_nn = {\n",
    "            \"n_inputs\" : self.num_features,\n",
    "            \"n_outputs\": self.num_class,\n",
    "            \"n_epochs\": self.n_epochs,\n",
    "            \"n_steps\": self.n_steps,\n",
    "            \"batchnorm\": self.batchnorm,\n",
    "            \"hidden_size\": self.hidden_size,\n",
    "            \"max_batch\": self.max_batch\n",
    "            }        \n",
    "        if self.bayes == True:\n",
    "            config_nn[\"batches\"], config_nn[\"lr\"], config_nn[\"layers\"]  = self.batches_bayes, self.lr_bayes, self.layers_bayes\n",
    "            config_nn[\"rep\"], config_nn[\"set\"], config_nn[\"n_val_splits\"]  = self.rep, self.set, self.n_val_splits\n",
    "            config_nn[\"cval\"], config_nn[\"val_sets\"], config_nn[\"n_trials\"] = self.cval, self.val_sets, self.n_trials      \n",
    "            bayes = cslr_bayes(x_train_nn, r_train_nn, config_nn)\n",
    "            best_params_bayes = bayes.bayes()\n",
    "            best_lr = best_params_bayes.get(\"lr_rate\", \"\"), \n",
    "            best_batch_size = best_params_bayes.get(\"batch_size\" \"\")\n",
    "            best_layer = best_params_bayes.get(\"num_ayer\" \"\")\n",
    "            config_nn[\"dnn_layers\"] = best_layer         \n",
    "            config_nn[\"batch_size\"] = (2**best_batch_size if best_batch_size<self.max_batchsize else len(x_train_nn))\n",
    "        else:\n",
    "            best_lr = self.lr_rate\n",
    "            config_nn[\"batch_size\"] = self.batch_size\n",
    "            config_nn[\"dnn_layers\"] = self.numlayer\n",
    "        self.model = LinearNet(config_nn).to(self.device)\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=best_lr)\n",
    "        optimization = Optimization(self.model, optimizer, config_nn)  \n",
    "        optimization.train(x_train_nn, r_train_nn, x_test_nn, r_test_nn)\n",
    "        _, _, test_probs = optimization.evaluate(x_test_nn, r_test_nn)\n",
    "        _, _, train_probs = optimization.evaluate(x_train_nn, r_train_nn)\n",
    "        if self.x_val is not None:\n",
    "            x_val_nn = torch.Tensor(self.x_val).to(self.device)\n",
    "            r_val_nn = torch.Tensor(self.r_val).to(self.device)                \n",
    "            _, _, val_probs = optimization.evaluate(x_val_nn, r_val_nn)\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if \"layer\" in name:\n",
    "                    self.beta_initial = param.data        \n",
    "        return((test_probs, train_probs, val_probs) if self.x_val is not None else (test_probs, train_probs))\n",
    "    \n",
    "    def ret_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def mip_opt(self):\n",
    "    \n",
    "        if self.give_initial == True:\n",
    "            if self.beta_initial is None:\n",
    "                self.gradient()\n",
    "            x_train, r_train = self.opt_initial(self.beta_initial)\n",
    "            model = Optimization_MIP({}, x_train, r_train)\n",
    "            for i in range(self.num_class):\n",
    "                for j in range(self.num_features):\n",
    "                    model.beta[i,j].start = self.beta_initial[i,j]\n",
    "            model.m.update() \n",
    "        else:\n",
    "            model = Optimization_MIP({}, self.x_train, self.r_train)\n",
    "        model.m.modelSense = GRB.MAXIMIZE\n",
    "        model.m.setParam(GRB.Param.TimeLimit, self.time_limit)\n",
    "        model.m.optimize()\n",
    "        test_probs = np.zeros((len(self.x_test), self.num_class))\n",
    "        train_probs = np.zeros((len(self.x_train), self.num_class))        \n",
    "        try:\n",
    "            for i in range(len(self.x_test)):\n",
    "                for k in range(self.num_class):\n",
    "                    test_probs[i,k] = sum(self.x_test[i,j] * model.beta[k,j].x for j in range(self.num_features))\n",
    "            for i in range(len(self.x_train)):\n",
    "                for k in range(self.num_class):\n",
    "                    train_probs[i,k] = sum(self.x_train[i,j] * model.beta[k,j].x for j in range(self.num_features))  \n",
    "            if self.x_val is not None:\n",
    "                val_probs = np.zeros((len(self.x_val), self.num_class))       \n",
    "                for i in range(len(self.x_val)):\n",
    "                    for k in range(self.num_class):\n",
    "                        val_probs[i,k] = sum(self.x_val[i,j] * model.beta[k,j].x for j in range(self.num_features))  \n",
    "            return((test_probs, train_probs, val_probs) if self.x_val is not None else (test_probs, train_probs))\n",
    "        except:\n",
    "             return((test_probs, train_probs, val_probs) if self.x_val is not None else (test_probs, train_probs))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
